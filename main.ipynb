{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "02c54141",
      "metadata": {
        "id": "02c54141"
      },
      "source": [
        "## Spam Email Classifier with KNN using TF-IDF scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d34a310",
      "metadata": {
        "id": "0d34a310"
      },
      "source": [
        "**Task: Given an email, classify it as spam or ham**\n",
        "\n",
        "Given input text file (\"emails.txt\") containing 5572 email messages, with each row having its corresponding label (spam/ham) attached to it.\n",
        "\n",
        "This task also requires basic pre-processing of text (like removing stopwords, stemming/lemmatizing, replacing email_address with 'email-tag', etc..).\n",
        "\n",
        "You are required to find the tf-idf scores for the given data and use them to perform KNN using Cosine Similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c87696",
      "metadata": {
        "id": "b0c87696"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5a1fe2",
      "metadata": {
        "id": "3d5a1fe2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json \n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "Lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYW5YSVXhcDL",
        "outputId": "472d487a-8d87-49a5-991b-f011bf5c548f"
      },
      "id": "WYW5YSVXhcDL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aef4dff",
      "metadata": {
        "id": "7aef4dff"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f178f892",
      "metadata": {
        "id": "f178f892"
      },
      "outputs": [],
      "source": [
        "f = open('emails.txt', 'r')\n",
        "data = []\n",
        "tags = []\n",
        "for sentence in f:\n",
        "  # print(sentence.strip())\n",
        "  # print(sentence.split()[0])\n",
        "  tags.append(sentence.split()[0])\n",
        "  # print(sentence.split()[1:])\n",
        "  # print(\" \".join(sentence.strip().lower().split()[1:]))\n",
        "  # data.append(\" \".join(sentence.strip().lower().split()[1:]))\n",
        "  words = sentence.strip().lower().split()[1:]\n",
        "  new_words = [Lemmatizer.lemmatize(word) for word in words]\n",
        "  data.append(\" \".join(new_words))\n",
        "# print(len(tags), len(data))\n",
        "# print(data[:5])\n",
        "# print(tags[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd1ef5ba",
      "metadata": {
        "id": "cd1ef5ba"
      },
      "source": [
        "### Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1733d7",
      "metadata": {
        "id": "fd1733d7"
      },
      "outputs": [],
      "source": [
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~+='''\n",
        "process_data = []\n",
        "for sentence in data:\n",
        "  new_sentence = \" \"\n",
        "  for word in sentence:\n",
        "    if word in punctuations:\n",
        "      new_sentence = sentence.replace(word, \" \")\n",
        "  process_data.append(new_sentence)\n",
        "# for i,_ in enumerate(process_data):\n",
        "#   print(_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f76767a7",
      "metadata": {
        "id": "f76767a7"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75e6cd2",
      "metadata": {
        "id": "f75e6cd2"
      },
      "outputs": [],
      "source": [
        "train_data = process_data[:5000]\n",
        "train_labels = tags[:5000]\n",
        "test_data = process_data[5000:]\n",
        "test_labels = tags[5000:]\n",
        "# print(len(train_data))\n",
        "# print(len(train_labels), len(test_data), len(test_labels))\n",
        "# print(train_data[-1])\n",
        "# print(test_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6eb76b",
      "metadata": {
        "id": "ee6eb76b"
      },
      "source": [
        "### Train your KNN model (reuse previously iplemented model built from scratch) and test on your data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = list()\n",
        "for sentence in process_data:\n",
        "  words = sentence.split()\n",
        "  for word in words:\n",
        "    corpus.append(word)\n",
        "# print(len(corpus))\n",
        "vocab_len = len(corpus)"
      ],
      "metadata": {
        "id": "DLsgGAvYlKRy"
      },
      "id": "DLsgGAvYlKRy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores = {}\n",
        "tf_scores = {}\n",
        "for word in corpus:\n",
        "  if word in df_scores:\n",
        "    df_scores[word] += 1\n",
        "  else:\n",
        "    df_scores[word] = 1\n",
        "df_scores = {a: v for a, v in sorted(df_scores.items(), key = lambda item: item[1], reverse = True)}\n",
        "for key in df_scores:\n",
        "  tf_scores[key] = df_scores[key]/vocab_len\n",
        "# print(tf_scores)\n",
        "idf_scores = {}\n",
        "for key in df_scores:\n",
        "  idf_scores[key] = vocab_len/df_scores[key]\n",
        "# print(idf_scores)"
      ],
      "metadata": {
        "id": "fqy4lL-9mz3Q"
      },
      "id": "fqy4lL-9mz3Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E8p01wUY3iBg"
      },
      "id": "E8p01wUY3iBg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}